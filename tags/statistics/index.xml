<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Statistics on Axiomize</title>
    <link>https://WooHyeok-Moon.github.io/tags/statistics/</link>
    <description>Recent content in Statistics on Axiomize</description>
    <image>
      <title>Axiomize</title>
      <url>https://WooHyeok-Moon.github.io/papermod-cover.png</url>
      <link>https://WooHyeok-Moon.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr</language>
    <lastBuildDate>Tue, 11 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://WooHyeok-Moon.github.io/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI대학원 면접 준비</title>
      <link>https://WooHyeok-Moon.github.io/posts/ai_0/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://WooHyeok-Moon.github.io/posts/ai_0/</guid>
      <description>Statistics 1. Central Limit Theorem 표본의 개수 $n$이 충분히 클 때 표본 분포가 정규분포 $N(\mu, \sigma^2)$에 근사하는 것
$E(\bar{x}) \rightarrow \mu$
Deep $\cdot$ Machine Learning 1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.
해결 방법: 규제(Regulerization)
과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.
Ridge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.
L1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.</description>
    </item>
    
  </channel>
</rss>
