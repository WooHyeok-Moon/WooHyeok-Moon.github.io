<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deep Learning on Axiomize</title>
    <link>https://WooHyeok-Moon.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Axiomize</description>
    <image>
      <title>Axiomize</title>
      <url>https://WooHyeok-Moon.github.io/papermod-cover.png</url>
      <link>https://WooHyeok-Moon.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr</language>
    <lastBuildDate>Tue, 11 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://WooHyeok-Moon.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI대학원 면접 준비</title>
      <link>https://WooHyeok-Moon.github.io/posts/ml_ex/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://WooHyeok-Moon.github.io/posts/ml_ex/</guid>
      <description>1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.
해결 방법: 규제(Regulerization)
과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.
Ridge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.
L1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.
L2 norm은 제곱 연산이 들어있어 이상치에 민감하다.
L1 norm은 절댓값이 취해져 있어 0에서 미분불가능하다
Gradient Descent 손실함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복한다.</description>
    </item>
    
  </channel>
</rss>
