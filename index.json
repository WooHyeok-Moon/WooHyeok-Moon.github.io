[{"content":"Lecture 1 상대도수 해석 집합론 집합론을 확률에 적용 확률의 공리 Lecture 2 조건부 확률 전체 확률의 법칙(전확률 정리) 베이즈 정리 통적 독립 베르누이 시행 시스템 신뢰도(직렬, 병렬) Lecture 3 베르누이 시행, 이항분포, 포아송 분포의 관계 드므와브르-라플라스 정리 Lecture 4 확률변수의 정의 확률변수 예시 누적분포함수 확률밀도함수 확률질량함수 연속확률변수 이산확률변수 Lecture 5 이변량 확률변수 확률벡터 복소수 확률변수 조건부 확률분포/밀도 확률변수의 독립 확률변수의 함수 확률벡터의 함수 Lecture 6 확률변수의 기댓값 확률변수의 적률 확률변수에서 uncorrelated와 orthogonal의 의미 마코브 부등식 체비셰프 부등식 적률생성함수와 특성함수 확률벡터의 특성함수 특성함수 예시 Lecture 7 조건부 기댓값 기댓값 예측의 활용 통신이론 예시 선형최소제곱평균오차(LMMSE) 추정 확률벡터에서의 LMMSE 추정 Lecture 8 기댓값의 벡터 및 행렬 상관행렬, 공분산행렬 교차상관행렬, 교차공분산행렬 일변량 정규확률변수 이변량 정규확률변수 다변량 정규확률변수 Lecture 9 (pending) Lecture 10 (pending) ","permalink":"https://WooHyeok-Moon.github.io/posts/rp0/","summary":"Lecture 1 상대도수 해석 집합론 집합론을 확률에 적용 확률의 공리 Lecture 2 조건부 확률 전체 확률의 법칙(전확률 정리) 베이즈 정리 통적 독립 베르누이 시행 시스템 신뢰도(직렬, 병렬) Lecture 3 베르누이 시행, 이항분포, 포아송 분포의 관계 드므와브르-라플라스 정리 Lecture 4 확률변수의 정의 확률변수 예시 누적분포함수 확률밀도함수 확률질량함수 연속확률변수 이산확률변수 Lecture 5 이변량 확률변수 확률벡터 복소수 확률변수 조건부 확률분포/밀도 확률변수의 독립 확률변수의 함수 확률벡터의 함수 Lecture 6 확률변수의 기댓값 확률변수의 적률 확률변수에서 uncorrelated와 orthogonal의 의미 마코브 부등식 체비셰프 부등식 적률생성함수와 특성함수 확률벡터의 특성함수 특성함수 예시 Lecture 7 조건부 기댓값 기댓값 예측의 활용 통신이론 예시 선형최소제곱평균오차(LMMSE) 추정 확률벡터에서의 LMMSE 추정 Lecture 8 기댓값의 벡터 및 행렬 상관행렬, 공분산행렬 교차상관행렬, 교차공분산행렬 일변량 정규확률변수 이변량 정규확률변수 다변량 정규확률변수 Lecture 9 (pending) Lecture 10 (pending) ","title":"랜덤 프로세스 내용"},{"content":"데이터 요약(summarization)은 데이터 volume을 줄이는 효과 뿐 아니라, 복잡한 데이터로부터 패턴, 추세, 이상징후와 같은 insight를 추출해낼 수 있다.\n1) 대표값, 중심 측도(Measure of Location) 대표값을 통해 중요한 insight를 얻을 수 있다. 이러한 대표값들은 데이터의 변화, 영향 및 분석 결과를 평가하는 기준을 제공한다.\n1.1) 평균(Arithmetic Mean) 1.1.1) 정의 및 계산 값들을 모두 더한 뒤 sample 수로 나누어준다.\n$$\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n}$$\n1.1.2) 그룹화된 데이터 $k$개의 그룹으로 나뉜 데이터의 전체 평균은, 각 그룹의 평균에 그룹별 sample size만큼 가중치를 주어 계산한다.\n$$\\bar{X} = \\sum_{i=1}^{k} \\bar{X_i} \\frac{n_i}{n}$$\n위와 같은 응용을 통해 데이터를 더 세밀하게 분석할 수 있다.\n1.1.3) 개별 관측치의 영향력 평균에서는 이상치의 영향력이 매우 크다.\n$$ \\bar{X} = \\bar{X}_{(j)} \\frac{(n-1)}{n} + X_j \\cdot \\frac{1}{n} = \\bar{X}_{(j)} + (X_j - \\bar{X}_{(j)}) \\cdot \\frac{1}{n} $$\n평균은 이상치에 취약하고 다른 데이터들의 유의성을 떨어트리기 때문에 개별적인 처리가 필요하다. 하지만 홍수나 가뭄과 같은 데이터에서는 이상치처럼 보일 수 있는 데이터가 실제로 유효하고 중요한 데이터인 경우가 많기 때문에 처리에 유의해야 한다. 위 그림처럼 비정상적으로 큰 하나의 데이터가 평균에 큰 영향을 미친다. 마치 지렛대와 비슷해보임..\n1.1.4) 생각해볼 것 평균을 통해 데이터의 핵심적인 정보를 얻을 수 있지만, 수질 데이터와 같이 자연적, 인위적 영향으로 인한 이상치가 존재할 수 있는 데이터에서는 전적으로 신뢰해서는 안 된다. 수역으로 유입되는 총 성분과 같이 데이터의 전체 합에 관심 있는 경우에는 평균이 정말 좋은 대표값이지만, 일반적인 경우 중앙값이나 최빈값이 outlier에 더 robust하다. 1.2) 중앙값(The Median) 1.2.1) 정의 및 계산 중앙값은 데이터를 sorting하여 나열했을 때 중간 위치에 해당하는 값이다.\n$$ median = P_{0.50} = \\begin{cases} X(\\frac{n+1}{2}) \u0026amp;\\text{when $n$ is odd}\\\\ \\frac{1}{2}\\Big(X(\\frac{n}{2})+X(\\frac{n}{2}+1)\\Big) \u0026amp;\\text{when $n$ is even} \\end{cases} $$\nsample size가 짝수인 경우 중간 두 값의 평균으로 계산된다. 중앙값은 정렬된 데이터에서 순서를 기준으로 데이터를 추출하기 때문에 이상치에 robust하다. 1.2.2) 이상치에 대한 저항 import numpy as np A = np.array([2, 4, 8, 9, 11, 11, 12]) B = np.array([2, 4, 8, 9, 11, 11, 120]) print(f\u0026#39;median of A: {np.median(A)}\u0026#39;) print(f\u0026#39;median of B: {np.median(B)}\u0026#39;) median of A: 9.0 median of B: 9.0 위 예시를 보면 B에 120이라는 이상치가 들어있으나 중앙값은 그대로 9가 튀어나온다.\n1.2.3) 특정 상황에서 평균보다 선호되는 이유 데이터를 요약할 때, 일반적으로 평균보다 중앙값이 안정적인 값을 제공한다. 예를 들어 다양한 개울의 화학 물질 농도를 측정한다고 할 때, 중앙값을 사용하면 농도가 비정상적으로 높은 하나의 개울이 있다고 하더라도 전체적인 추정치에 크게 영향을 미치지 않는다. 1.3) 중심을 측정하는 다른 대표값들 1.3.1) 최빈값(Mode) 정의: 데이터에서 가장 자주 나온 값을 의미하며, 1nominal data에서 사용할 수 있는 유일한 대표값이다. land cover type과 같이 일반적인 인간의 행동이나 선호도를 이해하는 데에 유용하다. 하나의 dataset에 봉우리가 하나 존재하면 unimodal, 둘 존재하면 bimodal, 이보다 여러 개 존재하면 multimodal이라고 한다. 1.3.2) 기하평균(Geometric Mean) 정의: 각 샘플을 모두 곱해서 sample size만큼 n차 제곱근을 취한다. $$ GM = \\sqrt[n]{X_1 \\cdot X_2 \\cdot \\ldots \\cdot X_n} = \\Big(\\prod_{i=1}^{n} x_i\\Big)^{\\frac{1}{n}} $$\n데이터의 로그 평균을 씌워 구한 다음 원래 단위로 변환하면 더 간단하게 구할 수 있다. $$ GM = exp\\bigg(\\frac{\\sum_{i=1}^{n} Y_i}{n}\\bigg) = exp(\\bar{Y}) $$\n기하평균은 모든 데이터가 양수인 상황에서만 정의된다.\n1.3.3) 가중평균(Weighted Mean) 정의: 각각의 데이터에 대한 상대적인 중요도를 반영한 평균 $$ \\bar{x}_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^n w_i} $$\n예를 들어 수질 오염 데이터가 있다고 하면 수질이 오염되도록 하는 다양한 오염원들이 있을텐데, 어떤 오염원은 다른 오염원에 비해 수질을 악화시키는 정도가 크다. 이처럼 feature들이 target에 미치는 상대적인 영향을 반영할 수 있다. 이러한 가중치를 적절하게 부여하는 것은 데이터의 특성을 정확하게 반영하는 데에 큰 도움이 된다. 2) 변동성 측정 데이터의 퍼진 정도를 수치화한 것\n환경 데이터에서 퍼진 정도를 이해하는 것은 생태계의 변동성, 인간 활동이 환경에 미치는 영향, 환경을 보존하려는 노력의 효과 등을 정량적으로 해석하는 데에 큰 도움이 된다.\n2.1) 고전적인 방법들 2.1.1) 범위 가장 단순한 형태의 변동성 측정 방법으로, 특정 계절의 온도 범위와 같이 양 극단 사이의 범위를 나타낸다. $$ Range = Max(X) - Min(X) $$\n산불이나 화재와 같은 extreme한 사건이 존재하는 데이터에서, 해석에 부정적인 영향을 끼칠 수 있다.\n2.1.2) 분산 평균에 편차제곱(squared deviation)을 취하여 퍼진 정도를 구할 수 있다. $$ s^2 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}{n-1} $$\n수자원 확보를 위한 하천 방류량의 변동성 측정에 사용할 수 있음\n환경 데이터는 skewed data인 경우가 많은데, 이 경우 변환을 한 번 거쳐야 하기 때문에 데이터가 정규분포를 따를 때 효율적이다.\n2.1.3) 표준편차 분산은 단위가 데이터의 제곱 꼴이기 때문에 root를 씌워 조금 더 이해하기 쉬운 단위로 만들어준다.\n$$ s = \\sqrt{s^2} $$\n수치형 데이터와 달리 하나의 이름에 데이터를 분류할 수 있는 데이터. e.g.) 홍팀 청팀 백팀\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://WooHyeok-Moon.github.io/posts/si2/","summary":"데이터 요약(summarization)은 데이터 volume을 줄이는 효과 뿐 아니라, 복잡한 데이터로부터 패턴, 추세, 이상징후와 같은 insight를 추출해낼 수 있다.\n1) 대표값, 중심 측도(Measure of Location) 대표값을 통해 중요한 insight를 얻을 수 있다. 이러한 대표값들은 데이터의 변화, 영향 및 분석 결과를 평가하는 기준을 제공한다.\n1.1) 평균(Arithmetic Mean) 1.1.1) 정의 및 계산 값들을 모두 더한 뒤 sample 수로 나누어준다.\n$$\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n}$$\n1.1.2) 그룹화된 데이터 $k$개의 그룹으로 나뉜 데이터의 전체 평균은, 각 그룹의 평균에 그룹별 sample size만큼 가중치를 주어 계산한다.","title":"기초통계"},{"content":"Basics of Environmental Data 1) 환경 데이터 특징 1.1) 시계열 데이터 (Time-series data) continous한 시간 간격에 따른 데이터. 시간, 일 또는 월 별 데이터와 같이 규칙적일 수도 있고, interval 자체가 불규칙적일 수도 있다.\n예시:\n계절, 연 단위의 강우량 변화 강, 호수, 댐 등의 수위 호수나 시냇물의 유량 pH, 혼탁도, 오염도 등과 같은 수질 시계열 데이터 특성:\n시간 종속성(Temporal dependency): 일반적인 데이터와 다르게 시계열 데이터는 고려해야 할 X(독립 변수) 시간 경과에 따른 특징이 존재하며, 이를 통해 유용한 정보를 얻을 수 있다. 하지만 이러한 특징이 결측치, 노이즈, 계절성, 추세 및 인과관계 처리와 같은 feature engineering에 어려움을 줄 수도 있다. (How to handle temporal dependencies in feature engineering) 계절성(Seasonality): 계절에 따른 반복성 또는 주기가 존재한다. 추세(Trend): 특정 지역의 지하수가 점진적으로 고갈되는 것과 같이, 시간 경과에 따른 데이터의 장기적인 추세가 존재한다. 1.2) 공간 데이터(Spatial Data) 지리적, 공간적 정보를 포함한 데이터. 지구상의 특정한 위치에서 일어나는 현상을 표현하고 분석하는 데에 쓰인다.\n예시:\n강, 호수 및 대수층($\\approx$지하수층, \u0026lsquo;aquifers\u0026rsquo; in English) 각 토지의 쓰임새와 그로 인해 발생하는 주변 수질 또는 수위의 변화 1infrastructure 건설 시의 기대 효과 분석 토지의 침식 및 퇴적 공간 데이터 특성:\n위치(Location)는 좌표(위도 및 경도) 또는 기타 지리적 척도를 통해 표현된다. Attribute(설명변수) 정보는 2LULC(Land Use Land Cover)나 수질을 통해 표현한다. (보통 map에 color로 구분) 날짜의 경과에 따른 해수면의 변화 공간 관계성: 각 지역 간 서로 어떤 관계에 있는지를 나타내는 정보. e.g. 인접(adjacency): 서로 접해있는가, 포함(containment): A 지역이 B 지역을 포함하고 있는가 1.3) 시공간 데이터(Spatio-Temporal Data) 시간적, 공간적 정보를 모두 담고 있는 데이터. 시공간에 영향을 받는 환경 현상을 역동적으로 보여준다. 복잡한 환경 현상을 이해하는 데에 유용하다.\n예시:\n계절, 연도별 강수량 패턴. 현재 NASA에서는 3GPM을 수행하고 있다. 시간 경과에 따른 토지 사용 방식의 변화 시간 경과에 따른 각 유역(watershed)의 수질 변화 수온 및 수질 변화에 따른 수상생물의 계절 단위 이동 시공간 데이터 특성:\nDynamic Location Information: 특정 현상이 시간이 지남에 따라 어디로 이동하는지 역동적으로 파악 가능하다. (focus on phenomenon) Temporal Changes: 특정 지역에서 시간이 지남에 따라 어떤 현상이 발생하는지 파악 가능하다. (focus on location) Complex Data Structure: 시간과 공간이 모두 포함된 고차원 데이터이기 때문에 정교한 분석 기법이 필요하다. footnote 사회적 생산 기반. 또는, 경제 활동의 기반을 형성하는 기초적인 시설. 댐·도로·항만·발전소·통신 시설 등의 산업 기반 및 학교·병원·공원 등의 사회 복지·환경 시설이 이에 해당함. 인프라.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n어떤 종류의 토지가 어떻게 쓰이는지를 나타내는 지도 = LULC map\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPM(Global Precipitation Measurement Mission): 인공위성을 통해 전 세계의 강수량을 측정하는 mission\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://WooHyeok-Moon.github.io/posts/si1/","summary":"Basics of Environmental Data 1) 환경 데이터 특징 1.1) 시계열 데이터 (Time-series data) continous한 시간 간격에 따른 데이터. 시간, 일 또는 월 별 데이터와 같이 규칙적일 수도 있고, interval 자체가 불규칙적일 수도 있다.\n예시:\n계절, 연 단위의 강우량 변화 강, 호수, 댐 등의 수위 호수나 시냇물의 유량 pH, 혼탁도, 오염도 등과 같은 수질 시계열 데이터 특성:\n시간 종속성(Temporal dependency): 일반적인 데이터와 다르게 시계열 데이터는 고려해야 할 X(독립 변수) 시간 경과에 따른 특징이 존재하며, 이를 통해 유용한 정보를 얻을 수 있다.","title":"환경 데이터 종류"},{"content":"마크다운에서 아래와 같이 식을 정렬하고 싶을 때가 있다.\n$$ f(x) = ax_1 + bx_2 = cx_3 + dx_4 = ex_5 + fx_6\\\\\\ \\downarrow\\\\\\ \\begin{align} f(x) \u0026amp;= ax_1 + bx_2\\\\\\ \u0026amp;= cx_3 + dx_4\\\\\\ \u0026amp;= ex_5 + fx_6\\\\\\ \\end{align} $$ $$ f(x) = ax_1 + bx_2 = cx_3 + dx_4 = ex_5 + fx_6\\\\ \\downarrow\\\\ \\begin{align} f(x) \u0026amp;= ax_1 + bx_2\\\\ \u0026amp;= cx_3 + dx_4\\\\ \u0026amp;= ex_5 + fx_6\\\\ \\end{align} $$\n그럼 이렇게 정렬된 식 우측에 (1), (2), $\\cdots$ 와 같이 번호가 부여되는데, 등호(=)기준으로 정렬해야 하는 상황에서는 별다른 문제가 없지만 아래와 같은 상황에서는 문제가 발생한다.\n$$ f(x) = ax_1 + bx_2 + cx_3 + dx_4 + ex_5 + fx_6\\\\\\ \\downarrow\\\\\\ \\begin{align} f(x) =\u0026amp; ax_1 + bx_2\\\\\\ \u0026amp;+ cx_3 + dx_4\\\\\\ \u0026amp;+ ex_5 + fx_6\\\\\\ \\end{align} $$ $$ f(x) = ax_1 + bx_2 + cx_3 + dx_4 + ex_5 + fx_6\\\\ \\downarrow\\\\ \\begin{align} f(x) =\u0026amp; ax_1 + bx_2\\\\ \u0026amp;+ cx_3 + dx_4\\\\ \u0026amp;+ ex_5 + fx_6\\\\ \\end{align} $$\n","permalink":"https://WooHyeok-Moon.github.io/posts/md1/","summary":"마크다운에서 아래와 같이 식을 정렬하고 싶을 때가 있다.\n$$ f(x) = ax_1 + bx_2 = cx_3 + dx_4 = ex_5 + fx_6\\\\\\ \\downarrow\\\\\\ \\begin{align} f(x) \u0026amp;= ax_1 + bx_2\\\\\\ \u0026amp;= cx_3 + dx_4\\\\\\ \u0026amp;= ex_5 + fx_6\\\\\\ \\end{align} $$ $$ f(x) = ax_1 + bx_2 = cx_3 + dx_4 = ex_5 + fx_6\\\\ \\downarrow\\\\ \\begin{align} f(x) \u0026amp;= ax_1 + bx_2\\\\ \u0026amp;= cx_3 + dx_4\\\\ \u0026amp;= ex_5 + fx_6\\\\ \\end{align} $$","title":"마크다운(markdown) KaTex 공식 정렬 시 번호 제거"},{"content":"러시아의 수학자 콜모고로프(Andrei Kolmogorov, 1903~1987)는 확률을 공리적으로 정의하고자 하였으며 다음과 같은 공리가 확률을 정의하는 필요충분조건이 됨을 보였다.\n확률의 공리 (1) 임의의 사건 $A$에 대하여 $P(A) \\ge 0$이다.\n(2) $P(S) = 1$이다.\n(3) 표본공간 $S$에 정의된 사건열 $A_1, A_2, \\cdots$ 가 있다고 하자. 이제 모든 $i \\ne j$에 대하여 $A_i \\cap A_j = \\varnothing$ 이면 $P(\\displaystyle\\bigcup_{i=1}^{\\infty}A_i = \\displaystyle\\sum_{i=1}^{\\infty}P(A_i)$이다.\n정리 1.1 두 개의 사건 $A$와 $B$에 대하여 다음과 같은 성질들이 성립한다.\n$P(A^c) = 1 - P(A)$ $P(\\varnothing) = 0$ $A \\subset B$이면 $P(A) \\le P(B)$이다. $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ 증명 $1 = P(S) = P(A \\cup A^c) = P(A) + P(A_c) (\\because$ 공리 (3)) $0 = 1 - P(S) = P(S^c) = P(\\varnothing) (\\because$ 공리 (2)와 (1)) $B = A \\cup (B \\cap A^c)$로 나타낼 수 있으며, $A \\cap (B \\cap A^c) = \\varnothing$이므로 공리 (1)과 (3)에 의해 $$ \\\\ P(B) = P(A) + P(B \\cap A^c) \\ge P(A) \\\\ $$ 가 성립한다. $P(A) = P(A \\cap B^c) + P(A \\cap B), \\quad P(B) = P(B \\cap A^c) + P(A \\cap B)$ 예 1.8 예 1.1의 실험에서 표본공간 $S$에 속한 8개의 점들이 모두 같은 확률 $\\frac{1}{8}$을 가졌다고 가정하고, 최소한 1개의 앞면이 나오는 사건을 $A$, 꼭 2개의 앞면이 나오는 사건을 $B$, 그리고 첫 번째 동전이 앞면이 나오는 사건을 $C$라고 하자. 그러면, $A$의 여사건은 $A^c = {TTT}$가 되며 $$ \\begin{align} P(A) = 1 - P(A^c) = 1 - \\frac{1}{8} = \\frac{7}{8} \\end{align} $$ 이 된다. 또 $$ \\begin{align} P(B \\cup C) = P({HHH, HHT, HTH, THH, HTT}) = \\frac{5}{8} \\end{align} $$ 인데, 이는 $$ \\begin{align} \\begin{split} P({HHT, HTH, THH}) \u0026amp;+ P({HHH, HHT, HTH, HTT})\\\\ \u0026amp;- P({HHT, HTH})\\\\ \u0026amp;= \\frac{3}{8} + \\frac{4}{8} - \\frac{2}{8} = \\frac{5}{8} \\end{split} \\end{align} $$ 와 같이 구할 수도 있다.\n예 1.9 예 1.5의 실험에서 표본공간 $S = {1, 2, 3, 4, 5, 6}$에 속한 6개의 점들이 모두 같은 확률 $\\frac{1}{6}$을 가졌다고 가정하자. 사건 $A$와 $B$를 $$ A = {2, 4, 6} \\qquad B = {3, 6} $$ 으로 표현하면, $A \\cap B = {6}, A \\cup B = {2, 3, 4, 6}$이 된다. 여기에서 $$ \\begin{align} P(A \\cup B) \u0026amp;= P(A) + P(B) - P(A \\cap B) \\\\ \u0026amp;=\\frac{3}{6} + \\frac{2}{6} - \\frac{1}{6} \\\\ \u0026amp;=\\frac{2}{3} \\end{align} $$ 와 같음을 알 수 있다.\n예 1.10 두 사건 $A$와 $B$가 표본공간 $S$에서 정의되어 있는데, 둘 중 적어도 한 사건이 일어날 확률이 0.3이고 $A$만 일어날 확률이 0.1이라고 하자. 이때 $P(A \\cup B) = P(B) + P(A \\cap B^c)$이므로 $P(B) = 0.3 - 0.1 = 0.2$가 된다.\n예 1.11 세 사건 $A, B, C$가 표본공간 $S$에서 정의되어 있을 때, 정리 1.1의 (4)에 의해 $$ \\begin{align} P(A \\cup B \\cup C) =\u0026amp; P((A \\cup B) \\cup C) \\\\ =\u0026amp; P(A \\cup B) + P(C) - P((A \\cup B) \\cap C) \\\\ =\u0026amp; P(A) + P(B) - P(A \\cap B) + P(C) \\\\ \\tag*{} \u0026amp;- P((A \\cap C) \\cup (B \\cap C)) \\\\ =\u0026amp; P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) \\\\ \\tag*{} \u0026amp;- P(B \\cap C) + P(A \\cap B \\cap C) \\\\ \\end{align} $$ 이므로 다음이 성립한다. $$ P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(B \\cap C) - P(A \\cap C) + P(A \\cap B \\cap C) $$\n","permalink":"https://WooHyeok-Moon.github.io/posts/sm2/","summary":"러시아의 수학자 콜모고로프(Andrei Kolmogorov, 1903~1987)는 확률을 공리적으로 정의하고자 하였으며 다음과 같은 공리가 확률을 정의하는 필요충분조건이 됨을 보였다.\n확률의 공리 (1) 임의의 사건 $A$에 대하여 $P(A) \\ge 0$이다.\n(2) $P(S) = 1$이다.\n(3) 표본공간 $S$에 정의된 사건열 $A_1, A_2, \\cdots$ 가 있다고 하자. 이제 모든 $i \\ne j$에 대하여 $A_i \\cap A_j = \\varnothing$ 이면 $P(\\displaystyle\\bigcup_{i=1}^{\\infty}A_i = \\displaystyle\\sum_{i=1}^{\\infty}P(A_i)$이다.\n정리 1.1 두 개의 사건 $A$와 $B$에 대하여 다음과 같은 성질들이 성립한다.\n$P(A^c) = 1 - P(A)$ $P(\\varnothing) = 0$ $A \\subset B$이면 $P(A) \\le P(B)$이다.","title":"확률의 정의"},{"content":"1. 확률이론 1.1 표본공간과 사건 키워드 실험: 어떤 현상의 관찰결과를 얻기 위한 과정 표본공간($S$): 모든 관찰 가능한 결과의 집합 사건: 표본공간의 부분집합 정의 1.1 사건 $A$와 $B$가 표본공간 $S$상에 정의되었다고 하자.\n사건 $A$와 $B$가 동시에 속하는 사건을 $A$와 $B$의 공통부분이라고 하고 $A \\cap B$라고 표기한다. 사건 $A$ 또는 $B$에 속하는 사건을 $A$와 $B$의 합이라고 하고 $A \\cup B$로 표기한다. 예 1.1 동전을 3회 던지는 실험에서 앞면을 $H$, 뒷면을 $T$로 표시하면 표본공간은 $$ S = {\\ HHH,\\ HHT,\\ HTH,\\ THH,\\ HTT,\\ THT,\\ TTH,\\ TTT\\ } $$ 이고, \u0026lsquo;2회 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT,\\ HTH,\\ THH\\ } $$ 이다.\n예 1.2 예 1.1의 실험에서 \u0026lsquo;앞면이 나오는 횟수\u0026rsquo;를 고려하면 표본공간은 $$ S = {\\ 0,\\ 1,\\ 2,\\ 3\\ } $$ 이 되고, 이때 \u0026lsquo;최소한 1번 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ 1,\\ 2,\\ 3\\ } $$ 이다.\n예 1.3 한 개의 동전을 뒷면이 나올 떄까지 던질 때의 표본공간은 $$ S = {\\ T,\\ HT,\\ HHT,\\ HHHT,\\ \\cdots\\ } $$ 가 된다. 이처럼 표본공간의 원소가 무한개일 수도 있다. 이때 \u0026lsquo;3회째 뒷면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT\\ } $$ 이다.\n예 1.4 어떤 기계의 수명시간에 대한 측정을 할 때, 가능한 관찰값은 음이 아닌 실수이다. 따라서 표본공간은 $$ S = {\\ x\\ |\\ 0\\ \\le\\ x\\ \u0026lt; \\infty\\ } $$ 여기서 \u0026lsquo;20시간 이상 작동하는 사건\u0026rsquo;은 $$ A = {\\ x\\ |\\ x\\ \\ge\\ 20 \\ } $$ 이다.\n예 1.5 주사위를 1회 던져 나오는 눈의 수를 관찰하였다고 할 때, 표본공간은 $$ S = {\\ 1,\\ 2,\\ 3,\\ 4,\\ 5,\\ 6,\\ }$$ 이 된다. 사건 $A$를 짝수의 눈이 나오는 경우로, $B$를 3의 배수가 나오는 경우로 정의하면, $$ A = {\\ 2,\\ 4,\\ 6\\ }\\quad B = {\\ 3,\\ 6\\ } $$ 으로 표현된다. 이 경우 $A$와 $B$의 공통부분과 합은 각각 $$ \\begin{align} A \\cap B \u0026amp;= {\\ 6\\ }\\\\ A \\cup B \u0026amp;= {\\ 2,\\ 3,\\ 4,\\ 6\\ }\\\\ \\end{align} $$\n정의 1.2 동일 표본공간 $S$상에 정의된 두 사건 $A$와 $B$의 공통부분이 없을 때, 즉 $A \\cap B = \\varnothing$이면 두 사건 $A$와 $B$는 상호배반이라고 한다.\n예 1.6 예 1.1에서 \u0026lsquo;최소한 한 번의 앞면\u0026rsquo;이 나오는 사건과 \u0026lsquo;모두 뒷면\u0026rsquo;이 나오는 사건은 공통부분이 없으므로 상호배반이다.\n정의 1.3 사건 $A$가 표본공간 $S$상에 정의되어 있을 때, $A$에 포함되지 않는 모든 $S$의 원소의 집합을 $A$의 여사건이라고 하며 $A^c$로 표기한다.\n예 1.7 예 1.5에서 \u0026lsquo;짝수의 눈\u0026rsquo;이 나오는 사건의 여사건은 \u0026lsquo;홀수의 눈\u0026rsquo;이 나오는 사건이 된다.\n두 개 이상의 사건들 사이의 관계가 때로는 복잡하며, 이때는 벤 다이어그램들을 활용하면 편리하다.\n$A$와 $B$의 합 $A \\cup B$와 공통부분 $A \\cap B$는 다음과 같다.\nshow code\rimport numpy as np from matplotlib import pyplot as plt from matplotlib import font_manager, rc import matplotlib font_name = font_manager.FontProperties(fname=\u0026#34;c:/Windows/Fonts/malgun.ttf\u0026#34;).get_name() rc(\u0026#39;font\u0026#39;, family=font_name) matplotlib.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False fig, ax = plt.subplots(1, 2, figsize=(8,2)) theta = np.linspace(0, 2 * np.pi, 1000) radius = 7.5 a1 = radius * np.cos(theta) - 5 a2 = radius * np.sin(theta) b1 = radius * np.cos(theta) + 5 b2 = radius * np.sin(theta) ax[0].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[0].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[0].fill_between(a1, a2, color = \u0026#39;gray\u0026#39;) ax[0].fill_between(b1, b2, color = \u0026#39;gray\u0026#39;) ax[0].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[0].set_title(r\u0026#34;A $\\cup$ B\u0026#34;) ax[0].text(-6, 0, \u0026#39;A\u0026#39;) ax[0].text(5, 0, \u0026#39;B\u0026#39;) ax[0].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) ax[1].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[1].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[1].fill_between(a1[a1\u0026gt;0], a2[a1\u0026gt;0], color = \u0026#39;gray\u0026#39;) ax[1].fill_between(b1[b1\u0026lt;0], b2[b1\u0026lt;0], color = \u0026#39;gray\u0026#39;) ax[1].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[1].set_title(r\u0026#34;A $\\cap$ B\u0026#34;) ax[1].text(-6, 0, \u0026#39;A\u0026#39;) ax[1].text(5, 0, \u0026#39;B\u0026#39;) ax[1].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) plt.show() 드 모르간 법칙 $$ (A \\cup B)^c = A^c \\cap B^c\\ (A \\cap B)^c = A^c \\cup B^c $$\nshow code\rfig, ax = plt.subplots(1, 2, figsize=(8,2)) theta = np.linspace(0, 2 * np.pi, 1000) radius = 7.5 a1 = radius * np.cos(theta) - 5 a2 = radius * np.sin(theta) b1 = radius * np.cos(theta) + 5 b2 = radius * np.sin(theta) ax[0].patch.set_facecolor(\u0026#39;gray\u0026#39;) ax[0].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[0].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[0].fill_between(a1, a2, color = \u0026#39;white\u0026#39;) ax[0].fill_between(b1, b2, color = \u0026#39;white\u0026#39;) ax[0].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[0].set_title(r\u0026#34;$(A \\cup B)^c = A^c \\cap B^c$\u0026#34;) ax[0].text(-6, 0, \u0026#39;A\u0026#39;) ax[0].text(5, 0, \u0026#39;B\u0026#39;) ax[0].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) ax[1].patch.set_facecolor(\u0026#39;gray\u0026#39;) ax[1].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[1].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[1].fill_between(a1[a1\u0026gt;0]-0.3, a2[a1\u0026gt;0], color = \u0026#39;white\u0026#39;) ax[1].fill_between(b1[b1\u0026lt;0], b2[b1\u0026lt;0], color = \u0026#39;white\u0026#39;) ax[1].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[1].set_title(r\u0026#34;$(A \\cap B)^c = A^c \\cup B^c$\u0026#34;) ax[1].text(-6, 0, \u0026#39;A\u0026#39;) ax[1].text(5, 0, \u0026#39;B\u0026#39;) ax[1].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) plt.show() ","permalink":"https://WooHyeok-Moon.github.io/posts/sm1/","summary":"1. 확률이론 1.1 표본공간과 사건 키워드 실험: 어떤 현상의 관찰결과를 얻기 위한 과정 표본공간($S$): 모든 관찰 가능한 결과의 집합 사건: 표본공간의 부분집합 정의 1.1 사건 $A$와 $B$가 표본공간 $S$상에 정의되었다고 하자.\n사건 $A$와 $B$가 동시에 속하는 사건을 $A$와 $B$의 공통부분이라고 하고 $A \\cap B$라고 표기한다. 사건 $A$ 또는 $B$에 속하는 사건을 $A$와 $B$의 합이라고 하고 $A \\cup B$로 표기한다. 예 1.1 동전을 3회 던지는 실험에서 앞면을 $H$, 뒷면을 $T$로 표시하면 표본공간은 $$ S = {\\ HHH,\\ HHT,\\ HTH,\\ THH,\\ HTT,\\ THT,\\ TTH,\\ TTT\\ } $$ 이고, \u0026lsquo;2회 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT,\\ HTH,\\ THH\\ } $$ 이다.","title":"표본공간과 사건"},{"content":"Statistics 1. Central Limit Theorem 표본의 개수 $n$이 충분히 클 때 표본 분포가 정규분포 $N(\\mu, \\sigma^2)$에 근사하는 것\n$E(\\bar{x}) \\rightarrow \\mu$\nDeep $\\cdot$ Machine Learning 1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.\n해결 방법: 규제(Regulerization)\n과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.\nRidge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.\nL1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.\nL2 norm은 제곱 연산이 들어있어 이상치에 민감하다.\nL1 norm은 절댓값이 취해져 있어 0에서 미분불가능하다\nGradient Descent 손실함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복한다.\n이때 x축은 weight, y축은 cost이다. cost가 최소가 되도록 weight값을 갱신하는 과정을 반복\n초기 형태인 Batch Gradient Descent는 한 번에 모든 데이터를 업데이트하기 때문에 대용량 데이터를 처리하기에 적합하지 않다.\nSthochastic Gradient Descent 그리하여 데이터셋을 batch 단위로 쪼개 학습하는 방법을 적용하게 된다. Batch Normalization internal covariate shift를 줄이기 위한 기법\ncovariate shift란 train set의 분포와 test set의 분포가 달라 제대로 예측하지 못하는 현상을 말함\ninternal covariate shift는 이러한 현상이 neural network 내부에서 일어나는 것을 말하는데, hidden layer에 입력으로 들어오는 데이터의 분포가 달라지는 것을 의미하며, layer가 깊을수록 심화될 수 있다.\n또한 이는 mini batch 기법에서는 batch마다 분포가 달라져 특히 성능에 critical한 문제가 될 수 있다.\n이 분포를 단순히 whitening하게 되면 편향이 사라지게 되어 유의미한 정보가 유실될 수 있다.\nbatch마다 평균과 분산을 계산하여 표준화하되 scale factor($\\gamma$)와 shift factor($\\beta$)를 적용시킨다.\n학습 시 batch normalization은 activation function으로 들어가기 전에 수행되는데, scale factor와 shift factor가 들어옴으로써 relu를 사용할 때 음수에 해당하는 부분이 모두 0으로 사라져버리는 것을 방지할 수 있고, sigmoid activation function을 사용할 때 비선형성이 사라지는 현상 또한 방지할 수 있고, normalization하기 전인 원래 형태에 대해서도 학습을 진행할 수 있어 backpropagation 또한 적용할 수 있다.\n추론 시 batch normalization은 테스트 데이터 하나에 대해 답을 내야 하므로 평균과 분산을 낼 수 없으므로 training 시 mini-batch들의 평균과 분산을 대신 사용한다.\nCNN(Convolution Neural Network) 기존의 DNN은 1차원 형태의 데이터를 사용하므로 2차원 형태의 이미지가 입력값으로 들어왔을 때에는 1차원으로 Flatten시켜줘야 한다. 이 과정에서 정보 손실이 일어난다. 또한 추상화 과정 없이 바로 연산을 진행하는데, 이로 인해 효율성이 저하된다.\nCNN은 이미지를 이미지 그대로 받아 공간적/지역적 정보를 유지할 수 있다.\n이미지는 이미지 전체보다는 특징적인 부분과 그 주변 픽셀들의 연관성에 집중해야 한다. CNN은 이를 위해 Convolution 연산을 사용한다.\n하나의 Convolution layer에는 Convolution과 Activation이 포함되어 있다. Activation Function으로는 RELU를 사용한다.\n이후 생성된 여러 개의 결과값에 Max Pooling을 적용하여 크기를 줄여준다.\n다시 Convolution, Activation, Pooling을 진행한다.\n나온 Feature data들을 Flatten하여 1차원 형태의 데이터로 만든다.\nFC(Fully connected) layer를 통과시켜 Softmax를 적용해주면 최종 output이 나온다.\nActivation Function 활성화 함수는 선형 분류기를 비선형 분류기로 만들어주는 역할을 한다.\n선형 함수를 사용하면 층을 깊게 쌓는 의미가 없다.\n활성화 함수 없이 feedforward를 진행하게 된다면 아래와 같이 진행된다. $$ \\begin{align} f(x) \u0026amp;= w \\times x\\\\ f(f(x)) \u0026amp;= w \\times w \\times x \u0026amp;= w^2x\\\\ f(f(f(x))) \u0026amp;= w \\times w \\times w \\times x \u0026amp;= w^3x\\\\ \u0026amp;\\dots \\end{align} $$\n이는 $y = ax$인 선형 함수에서 $a = w^3$인 선형 함수가 되었을 뿐이며 이는 weight가 $w^3$인 한 개 층으로도 네트워크를 구성할 수 있음을 의미한다. -\u0026gt; Deep Network의 의미가 없다.\nOthers 1. 이 분야를 선택한 이유 2. 학위 중 목표 3. 졸업 후 계획 4. 통계학을 전공해서 이 분야를 연구할 때의 장점 ","permalink":"https://WooHyeok-Moon.github.io/posts/ai_0/","summary":"Statistics 1. Central Limit Theorem 표본의 개수 $n$이 충분히 클 때 표본 분포가 정규분포 $N(\\mu, \\sigma^2)$에 근사하는 것\n$E(\\bar{x}) \\rightarrow \\mu$\nDeep $\\cdot$ Machine Learning 1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.\n해결 방법: 규제(Regulerization)\n과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.\nRidge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.\nL1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.","title":"AI대학원 면접 준비"},{"content":"test kr\n","permalink":"https://WooHyeok-Moon.github.io/posts/test/","summary":"test kr","title":""}]