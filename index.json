[{"content":"1. 확률이론 1.1 표본공간과 사건 키워드 실험: 어떤 현상의 관찰결과를 얻기 위한 과정 표본공간($S$): 모든 관찰 가능한 결과의 집합 사건: 표본공간의 부분집합 정의 1.1 사건 $A$와 $B$가 표본공간 $S$상에 정의되었다고 하자.\n사건 $A$와 $B$가 동시에 속하는 사건을 $A$와 $B$의 공통부분이라고 하고 $A \\cap B$라고 표기한다. 사건 $A$ 또는 $B$에 속하는 사건을 $A$와 $B$의 합이라고 하고 $A \\cup B$로 표기한다. 예 1.1 동전을 3회 던지는 실험에서 앞면을 $H$, 뒷면을 $T$로 표시하면 표본공간은 $$ S = {\\ HHH,\\ HHT,\\ HTH,\\ THH,\\ HTT,\\ THT,\\ TTH,\\ TTT\\ } $$ 이고, \u0026lsquo;2회 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT,\\ HTH,\\ THH\\ } $$ 이다.\n예 1.2 예 1.1의 실험에서 \u0026lsquo;앞면이 나오는 횟수\u0026rsquo;를 고려하면 표본공간은 $$ S = {\\ 0,\\ 1,\\ 2,\\ 3\\ } $$ 이 되고, 이때 \u0026lsquo;최소한 1번 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ 1,\\ 2,\\ 3\\ } $$ 이다.\n예 1.3 한 개의 동전을 뒷면이 나올 떄까지 던질 때의 표본공간은 $$ S = {\\ T,\\ HT,\\ HHT,\\ HHHT,\\ \\cdots\\ } $$ 가 된다. 이처럼 표본공간의 원소가 무한개일 수도 있다. 이때 \u0026lsquo;3회째 뒷면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT\\ } $$ 이다.\n예 1.4 어떤 기계의 수명시간에 대한 측정을 할 때, 가능한 관찰값은 음이 아닌 실수이다. 따라서 표본공간은 $$ S = {\\ x\\ |\\ 0\\ \\le\\ x\\ \u0026lt; \\infty\\ } $$ 여기서 \u0026lsquo;20시간 이상 작동하는 사건\u0026rsquo;은 $$ A = {\\ x\\ |\\ x\\ \\ge\\ 20 \\ } $$ 이다.\n예 1.5 주사위를 1회 던져 나오는 눈의 수를 관찰하였다고 할 때, 표본공간은 $$ S = {\\ 1,\\ 2,\\ 3,\\ 4,\\ 5,\\ 6,\\ }$$ 이 된다. 사건 $A$를 짝수의 눈이 나오는 경우로, $B$를 3의 배수가 나오는 경우로 정의하면, $$ A = {\\ 2,\\ 4,\\ 6\\ }\\quad B = {\\ 3,\\ 6\\ } $$ 으로 표현된다. 이 경우 $A$와 $B$의 공통부분과 합은 각각 $$ \\begin{align} A \\cap B \u0026amp;= {\\ 6\\ }\\\\ A \\cup B \u0026amp;= {\\ 2,\\ 3,\\ 4,\\ 6\\ }\\\\ \\end{align} $$\n정의 1.2 동일 표본공간 $S$상에 정의된 두 사건 $A$와 $B$의 공통부분이 없을 때, 즉 $A \\cap B = \\varnothing$이면 두 사건 $A$와 $B$는 상호배반이라고 한다.\n예 1.6 예 1.1에서 \u0026lsquo;최소한 한 번의 앞면\u0026rsquo;이 나오는 사건과 \u0026lsquo;모두 뒷면\u0026rsquo;이 나오는 사건은 공통부분이 없으므로 상호배반이다.\n정의 1.3 사건 $A$가 표본공간 $S$상에 정의되어 있을 때, $A$에 포함되지 않는 모든 $S$의 원소의 집합을 $A$의 여사건이라고 하며 $A^c$로 표기한다.\n예 1.7 예 1.5에서 \u0026lsquo;짝수의 눈\u0026rsquo;이 나오는 사건의 여사건은 \u0026lsquo;홀수의 눈\u0026rsquo;이 나오는 사건이 된다.\n두 개 이상의 사건들 사이의 관계가 때로는 복잡하며, 이때는 벤 다이어그램들을 활용하면 편리하다.\n$A$와 $B$의 합 $A \\cup B$와 공통부분 $A \\cap B$는 다음과 같다.\nshow code\rimport numpy as np from matplotlib import pyplot as plt from matplotlib import font_manager, rc import matplotlib font_name = font_manager.FontProperties(fname=\u0026#34;c:/Windows/Fonts/malgun.ttf\u0026#34;).get_name() rc(\u0026#39;font\u0026#39;, family=font_name) matplotlib.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False fig, ax = plt.subplots(1, 2, figsize=(8,2)) theta = np.linspace(0, 2 * np.pi, 1000) radius = 7.5 a1 = radius * np.cos(theta) - 5 a2 = radius * np.sin(theta) b1 = radius * np.cos(theta) + 5 b2 = radius * np.sin(theta) ax[0].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[0].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[0].fill_between(a1, a2, color = \u0026#39;gray\u0026#39;) ax[0].fill_between(b1, b2, color = \u0026#39;gray\u0026#39;) ax[0].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[0].set_title(r\u0026#34;A $\\cup$ B\u0026#34;) ax[0].text(-6, 0, \u0026#39;A\u0026#39;) ax[0].text(5, 0, \u0026#39;B\u0026#39;) ax[0].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) ax[1].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[1].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[1].fill_between(a1[a1\u0026gt;0], a2[a1\u0026gt;0], color = \u0026#39;gray\u0026#39;) ax[1].fill_between(b1[b1\u0026lt;0], b2[b1\u0026lt;0], color = \u0026#39;gray\u0026#39;) ax[1].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[1].set_title(r\u0026#34;A $\\cap$ B\u0026#34;) ax[1].text(-6, 0, \u0026#39;A\u0026#39;) ax[1].text(5, 0, \u0026#39;B\u0026#39;) ax[1].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) plt.show() 드 모르간 법칙 $$ (A \\cup B)^c = A^c \\cap B^c\\ (A \\cap B)^c = A^c \\cup B^c $$\nshow code\rfig, ax = plt.subplots(1, 2, figsize=(8,2)) theta = np.linspace(0, 2 * np.pi, 1000) radius = 7.5 a1 = radius * np.cos(theta) - 5 a2 = radius * np.sin(theta) b1 = radius * np.cos(theta) + 5 b2 = radius * np.sin(theta) ax[0].patch.set_facecolor(\u0026#39;gray\u0026#39;) ax[0].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[0].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[0].fill_between(a1, a2, color = \u0026#39;white\u0026#39;) ax[0].fill_between(b1, b2, color = \u0026#39;white\u0026#39;) ax[0].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[0].set_title(r\u0026#34;$(A \\cup B)^c = A^c \\cap B^c$\u0026#34;) ax[0].text(-6, 0, \u0026#39;A\u0026#39;) ax[0].text(5, 0, \u0026#39;B\u0026#39;) ax[0].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) ax[1].patch.set_facecolor(\u0026#39;gray\u0026#39;) ax[1].plot(a1, a2, color=\u0026#39;black\u0026#39;) ax[1].plot(b1, b2, color=\u0026#39;black\u0026#39;) ax[1].fill_between(a1[a1\u0026gt;0]-0.3, a2[a1\u0026gt;0], color = \u0026#39;white\u0026#39;) ax[1].fill_between(b1[b1\u0026lt;0], b2[b1\u0026lt;0], color = \u0026#39;white\u0026#39;) ax[1].set(xlim=(-17.5, 17.5), ylim=(-10, 10)) ax[1].set_title(r\u0026#34;$(A \\cap B)^c = A^c \\cup B^c$\u0026#34;) ax[1].text(-6, 0, \u0026#39;A\u0026#39;) ax[1].text(5, 0, \u0026#39;B\u0026#39;) ax[1].tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False) plt.show() ","permalink":"https://WooHyeok-Moon.github.io/posts/sm1/","summary":"1. 확률이론 1.1 표본공간과 사건 키워드 실험: 어떤 현상의 관찰결과를 얻기 위한 과정 표본공간($S$): 모든 관찰 가능한 결과의 집합 사건: 표본공간의 부분집합 정의 1.1 사건 $A$와 $B$가 표본공간 $S$상에 정의되었다고 하자.\n사건 $A$와 $B$가 동시에 속하는 사건을 $A$와 $B$의 공통부분이라고 하고 $A \\cap B$라고 표기한다. 사건 $A$ 또는 $B$에 속하는 사건을 $A$와 $B$의 합이라고 하고 $A \\cup B$로 표기한다. 예 1.1 동전을 3회 던지는 실험에서 앞면을 $H$, 뒷면을 $T$로 표시하면 표본공간은 $$ S = {\\ HHH,\\ HHT,\\ HTH,\\ THH,\\ HTT,\\ THT,\\ TTH,\\ TTT\\ } $$ 이고, \u0026lsquo;2회 앞면이 나오는 사건\u0026rsquo;은 $$ A = {\\ HHT,\\ HTH,\\ THH\\ } $$ 이다.","title":"표본공간과 사건"},{"content":"러시아의 수학자 콜모고로프(Andrei Kolmogorov, 1903~1987)는 확률을 공리적으로 정의하고자 하였으며 다음과 같은 공리가 확률을 정의하는 필요충분조건이 됨을 보였다.\n확률의 공리 (1) 임의의 사건 $A$에 대하여 $P(A) \\ge 0$이다.\n(2) $P(S) = 1$이다.\n(3) 표본공간 $S$에 정의된 사건열 $A_1, A_2, \\cdots$ 가 있다고 하자. 이제 모든 $i \\ne j$에 대하여 $A_i \\cap A_j = \\varnothing$ 이면 $P(\\displaystyle\\bigcup_{i=1}^{\\infty}A_i = \\displaystyle\\sum_{i=1}^{\\infty}P(A_i)$이다.\n정리 1.1 두 개의 사건 $A$와 $B$에 대하여 다음과 같은 성질들이 성립한다.\n$P(A^c) = 1 - P(A)$ $P(\\varnothing) = 0$ $A \\subset B$이면 $P(A) \\le P(B)$이다. $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ 증명 $1 = P(S) = P(A \\cup A^c) = P(A) + P(A_c) (\\because$ 공리 (3)) $0 = 1 - P(S) = P(S^c) = P(\\varnothing) (\\because$ 공리 (2)와 (1)) $B = A \\cup (B \\cap A^c)$로 나타낼 수 있으며, $A \\cap (B \\cap A^c) = \\varnothing$이므로 공리 (1)과 (3)에 의해 $$ \\\\ P(B) = P(A) + P(B \\cap A^c) \\ge P(A) \\\\ $$ 가 성립한다. $P(A) = P(A \\cap B^c) + P(A \\cap B), \\quad P(B) = P(B \\cap A^c) + P(A \\cap B)$ 예 1.8 예 1.1의 실험에서 표본공간 $S$에 속한 8개의 점들이 모두 같은 확률 $\\frac{1}{8}$을 가졌다고 가정하고, 최소한 1개의 앞면이 나오는 사건을 $A$, 꼭 2개의 앞면이 나오는 사건을 $B$, 그리고 첫 번째 동전이 앞면이 나오는 사건을 $C$라고 하자. 그러면, $A$의 여사건은 $A^c = {TTT}$가 되며 $$ \\begin{align} P(A) = 1 - P(A^c) = 1 - \\frac{1}{8} = \\frac{7}{8} \\end{align} $$ 이 된다. 또 $$ \\begin{align} P(B \\cup C) = P({HHH, HHT, HTH, THH, HTT}) = \\frac{5}{8} \\end{align} $$ 인데, 이는 $$ \\begin{align} \\begin{split} P({HHT, HTH, THH}) \u0026amp;+ P({HHH, HHT, HTH, HTT})\\\\ \u0026amp;- P({HHT, HTH})\\\\ \u0026amp;= \\frac{3}{8} + \\frac{4}{8} - \\frac{2}{8} = \\frac{5}{8} \\end{split} \\end{align} $$ 와 같이 구할 수도 있다.\n예 1.9 예 1.5의 실험에서 표본공간 $S = {1, 2, 3, 4, 5, 6}$에 속한 6개의 점들이 모두 같은 확률 $\\frac{1}{6}$을 가졌다고 가정하자. 사건 $A$와 $B$를 $$ A = {2, 4, 6} \\qquad B = {3, 6} $$ 으로 표현하면, $A \\cap B = {6}, A \\cup B = {2, 3, 4, 6}$이 된다. 여기에서 $$ \\begin{align} P(A \\cup B) \u0026amp;= P(A) + P(B) - P(A \\cap B) \\\\ \u0026amp;=\\frac{3}{6} + \\frac{2}{6} - \\frac{1}{6} \\\\ \u0026amp;=\\frac{2}{3} \\end{align} $$ 와 같음을 알 수 있다.\n예 1.10 두 사건 $A$와 $B$가 표본공간 $S$에서 정의되어 있는데, 둘 중 적어도 한 사건이 일어날 확률이 0.3이고 $A$만 일어날 확률이 0.1이라고 하자. 이때 $P(A \\cup B) = P(B) + P(A \\cap B^c)$이므로 $P(B) = 0.3 - 0.1 = 0.2$가 된다.\n예 1.11 세 사건 $A, B, C$가 표본공간 $S$에서 정의되어 있을 때, 정리 1.1의 (4)에 의해 $$ \\begin{align} P(A \\cup B \\cup C) =\u0026amp; P((A \\cup B) \\cup C) \\\\ =\u0026amp; P(A \\cup B) + P(C) - P((A \\cup B) \\cap C) \\\\ =\u0026amp; P(A) + P(B) - P(A \\cap B) + P(C) \\\\ \u0026amp;- P((A \\cap C) \\cup (B \\cap C)) \\\\ =\u0026amp; P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) \\\\ \u0026amp;- P(B \\cap C) + P(A \\cap B \\cap C) \\\\ \\end{align} $$ 이므로 다음이 성립한다. $$ P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(B \\cap C) - P(A \\cap C) + P(A \\cap B \\cap C) $$\n","permalink":"https://WooHyeok-Moon.github.io/posts/sm2/","summary":"러시아의 수학자 콜모고로프(Andrei Kolmogorov, 1903~1987)는 확률을 공리적으로 정의하고자 하였으며 다음과 같은 공리가 확률을 정의하는 필요충분조건이 됨을 보였다.\n확률의 공리 (1) 임의의 사건 $A$에 대하여 $P(A) \\ge 0$이다.\n(2) $P(S) = 1$이다.\n(3) 표본공간 $S$에 정의된 사건열 $A_1, A_2, \\cdots$ 가 있다고 하자. 이제 모든 $i \\ne j$에 대하여 $A_i \\cap A_j = \\varnothing$ 이면 $P(\\displaystyle\\bigcup_{i=1}^{\\infty}A_i = \\displaystyle\\sum_{i=1}^{\\infty}P(A_i)$이다.\n정리 1.1 두 개의 사건 $A$와 $B$에 대하여 다음과 같은 성질들이 성립한다.\n$P(A^c) = 1 - P(A)$ $P(\\varnothing) = 0$ $A \\subset B$이면 $P(A) \\le P(B)$이다.","title":"확률의 정의"},{"content":"Statistics 1. Central Limit Theorem 표본의 개수 $n$이 충분히 클 때 표본 분포가 정규분포 $N(\\mu, \\sigma^2)$에 근사하는 것\n$E(\\bar{x}) \\rightarrow \\mu$\nDeep $\\cdot$ Machine Learning 1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.\n해결 방법: 규제(Regulerization)\n과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.\nRidge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.\nL1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.\nL2 norm은 제곱 연산이 들어있어 이상치에 민감하다.\nL1 norm은 절댓값이 취해져 있어 0에서 미분불가능하다\nGradient Descent 손실함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복한다.\n이때 x축은 weight, y축은 cost이다. cost가 최소가 되도록 weight값을 갱신하는 과정을 반복\n초기 형태인 Batch Gradient Descent는 한 번에 모든 데이터를 업데이트하기 때문에 대용량 데이터를 처리하기에 적합하지 않다.\nSthochastic Gradient Descent 그리하여 데이터셋을 batch 단위로 쪼개 학습하는 방법을 적용하게 된다. Batch Normalization internal covariate shift를 줄이기 위한 기법\ncovariate shift란 train set의 분포와 test set의 분포가 달라 제대로 예측하지 못하는 현상을 말함\ninternal covariate shift는 이러한 현상이 neural network 내부에서 일어나는 것을 말하는데, hidden layer에 입력으로 들어오는 데이터의 분포가 달라지는 것을 의미하며, layer가 깊을수록 심화될 수 있다.\n또한 이는 mini batch 기법에서는 batch마다 분포가 달라져 특히 성능에 critical한 문제가 될 수 있다.\n이 분포를 단순히 whitening하게 되면 편향이 사라지게 되어 유의미한 정보가 유실될 수 있다.\nbatch마다 평균과 분산을 계산하여 표준화하되 scale factor($\\gamma$)와 shift factor($\\beta$)를 적용시킨다.\n학습 시 batch normalization은 activation function으로 들어가기 전에 수행되는데, scale factor와 shift factor가 들어옴으로써 relu를 사용할 때 음수에 해당하는 부분이 모두 0으로 사라져버리는 것을 방지할 수 있고, sigmoid activation function을 사용할 때 비선형성이 사라지는 현상 또한 방지할 수 있고, normalization하기 전인 원래 형태에 대해서도 학습을 진행할 수 있어 backpropagation 또한 적용할 수 있다.\n추론 시 batch normalization은 테스트 데이터 하나에 대해 답을 내야 하므로 평균과 분산을 낼 수 없으므로 training 시 mini-batch들의 평균과 분산을 대신 사용한다.\nCNN(Convolution Neural Network) 기존의 DNN은 1차원 형태의 데이터를 사용하므로 2차원 형태의 이미지가 입력값으로 들어왔을 때에는 1차원으로 Flatten시켜줘야 한다. 이 과정에서 정보 손실이 일어난다. 또한 추상화 과정 없이 바로 연산을 진행하는데, 이로 인해 효율성이 저하된다.\nCNN은 이미지를 이미지 그대로 받아 공간적/지역적 정보를 유지할 수 있다.\n이미지는 이미지 전체보다는 특징적인 부분과 그 주변 픽셀들의 연관성에 집중해야 한다. CNN은 이를 위해 Convolution 연산을 사용한다.\n하나의 Convolution layer에는 Convolution과 Activation이 포함되어 있다. Activation Function으로는 RELU를 사용한다.\n이후 생성된 여러 개의 결과값에 Max Pooling을 적용하여 크기를 줄여준다.\n다시 Convolution, Activation, Pooling을 진행한다.\n나온 Feature data들을 Flatten하여 1차원 형태의 데이터로 만든다.\nFC(Fully connected) layer를 통과시켜 Softmax를 적용해주면 최종 output이 나온다.\nActivation Function 활성화 함수는 선형 분류기를 비선형 분류기로 만들어주는 역할을 한다.\n선형 함수를 사용하면 층을 깊게 쌓는 의미가 없다.\n활성화 함수 없이 feedforward를 진행하게 된다면 아래와 같이 진행된다. $$ \\begin{align} f(x) \u0026amp;= w \\times x\\\\ f(f(x)) \u0026amp;= w \\times w \\times x \u0026amp;= w^2x\\\\ f(f(f(x))) \u0026amp;= w \\times w \\times w \\times x \u0026amp;= w^3x\\\\ \u0026amp;\\dots \\end{align} $$\n이는 $y = ax$인 선형 함수에서 $a = w^3$인 선형 함수가 되었을 뿐이며 이는 weight가 $w^3$인 한 개 층으로도 네트워크를 구성할 수 있음을 의미한다. -\u0026gt; Deep Network의 의미가 없다.\nOthers 1. 이 분야를 선택한 이유 2. 학위 중 목표 3. 졸업 후 계획 4. 통계학을 전공해서 이 분야를 연구할 때의 장점 ","permalink":"https://WooHyeok-Moon.github.io/posts/ai_0/","summary":"Statistics 1. Central Limit Theorem 표본의 개수 $n$이 충분히 클 때 표본 분포가 정규분포 $N(\\mu, \\sigma^2)$에 근사하는 것\n$E(\\bar{x}) \\rightarrow \\mu$\nDeep $\\cdot$ Machine Learning 1. L1, L2 Regulerization 과제: 과적합을 감소시켜야 한다.\n해결 방법: 규제(Regulerization)\n과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다.\nRidge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다.\nL1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다.","title":"AI대학원 면접 준비"},{"content":"test kr\n","permalink":"https://WooHyeok-Moon.github.io/posts/test/","summary":"test kr","title":""}]