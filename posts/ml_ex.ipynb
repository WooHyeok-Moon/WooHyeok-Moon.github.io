{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e072cbcd-62c0-4fff-80ec-e034c69c03a8",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Woohyeok Moon\"\n",
    "date: 2023-07-11\n",
    "title: AI대학원 면접 준비\n",
    "categories: Artificial Intelligence\n",
    "tags: [Machine Learning, Deep Learning]\n",
    "showtoc: false\n",
    "weight: 10\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b38c2-b001-44eb-b5ae-6ecb1e5065d1",
   "metadata": {},
   "source": [
    "## 1. L1, L2 Regulerization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a7adc-b5d7-4825-a5ab-9332c9e355bc",
   "metadata": {},
   "source": [
    "- 과제: 과적합을 감소시켜야 한다.\n",
    "- 해결 방법: 규제(Regulerization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8335696-18ad-4127-84b4-0532e78a825d",
   "metadata": {},
   "source": [
    "- 과적합을 감소시키기 위해서 규제라는 개념을 도입하게 되는데, 이렇게 규제가 적용된 regression에는 Ridge와 Lasso가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3baccd1-b6db-4959-b341-bb356ba34e41",
   "metadata": {},
   "source": [
    "- Ridge는 L2, Lasso는 L1 norm이 규제로써 작동되는 regression으로, L1 norm은 manhattan distance이고, L2 norm은 euclidean distance의 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce0a7e-2f89-4f9c-a856-c8f471ae82ec",
   "metadata": {},
   "source": [
    "- L1 norm은 특정 feature을 없앨 수 있어 feature selection에 유리하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a678ad-7d07-445f-8746-425005948294",
   "metadata": {
    "tags": []
   },
   "source": [
    "- L2 norm은 제곱 연산이 들어있어 이상치에 민감하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37486152-8c66-48d7-aaa7-c301070d7b17",
   "metadata": {},
   "source": [
    "- L1 norm은 절댓값이 취해져 있어 0에서 미분불가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5f11b-bc36-4c51-83f7-ec7328b25144",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a9d0f-6369-4079-8a44-f1bde74c1e7e",
   "metadata": {},
   "source": [
    "- 손실함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7067177-ce16-4e92-bff6-b6cb2e9c8cde",
   "metadata": {},
   "source": [
    "- 이때 x축은 weight, y축은 cost이다. cost가 최소가 되도록 weight값을 갱신하는 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a677670-1cf4-42ea-8ba3-4c0eee6eab82",
   "metadata": {},
   "source": [
    "- 초기 형태인 Batch Gradient Descent는 한 번에 모든 데이터를 업데이트하기 때문에 대용량 데이터를 처리하기에 적합하지 않다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06518e-1f31-4ad5-ba3d-455adf8d34d6",
   "metadata": {},
   "source": [
    "## Sthochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e935ea2-6041-4641-8046-5ea66c40aca6",
   "metadata": {},
   "source": [
    "- 그리하여 데이터셋을 batch 단위로 쪼개 학습하는 방법을 적용하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6c298-03fe-4165-8cb7-dd6e35d811e6",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e055c4-7750-4c2a-ad47-993b00c7e286",
   "metadata": {},
   "source": [
    "- internal covariate shift를 줄이기 위한 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025147b9-d8ab-49ef-b6be-a4c23a280f3b",
   "metadata": {},
   "source": [
    "- covariate shift란 train set의 분포와 test set의 분포가 달라 제대로 예측하지 못하는 현상을 말함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216772b-ead9-47d9-b30a-589b09acd608",
   "metadata": {},
   "source": [
    "- internal covariate shift는 이러한 현상이 neural network 내부에서 일어나는 것을 말하는데, hidden layer에 입력으로 들어오는 데이터의 분포가 달라지는 것을 의미하며, layer가 깊을수록 심화될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d27faf-abbf-49d1-b032-d4a29970a906",
   "metadata": {},
   "source": [
    "- 또한 이는 mini batch 기법에서는 batch마다 분포가 달라져 특히 성능에 critical한 문제가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d35b6-d7b7-42d8-bb9d-f8b462dbd2c2",
   "metadata": {},
   "source": [
    "- 이 분포를 단순히 whitening하게 되면 편향이 사라지게 되어 유의미한 정보가 유실될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853d146-d850-4c7c-8593-74b877459669",
   "metadata": {},
   "source": [
    "- batch마다 평균과 분산을 계산하여 표준화하되 scale factor($\\gamma$)와 shift factor($\\beta$)를 적용시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3c8d8-2db6-4c50-9a18-44b819963fdb",
   "metadata": {},
   "source": [
    "- 학습 시 batch normalization은 activation function으로 들어가기 전에 수행되는데, scale factor와 shift factor가 들어옴으로써 relu를 사용할 때 음수에 해당하는 부분이 모두 0으로 사라져버리는 것을 방지할 수 있고, sigmoid activation function을 사용할 때 비선형성이 사라지는 현상 또한 방지할 수 있고, normalization하기 전인 원래 형태에 대해서도 학습을 진행할 수 있어 backpropagation 또한 적용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa7faf-fc1f-400e-aedf-b221ef97567b",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 추론 시 batch normalization은 테스트 데이터 하나에 대해 답을 내야 하므로 평균과 분산을 낼 수 없으므로 training 시 mini-batch들의 평균과 분산을 대신 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f11567-f5fd-4896-8dbe-dec18c491f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
